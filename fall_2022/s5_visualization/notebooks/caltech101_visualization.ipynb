{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "from functools import reduce\n",
    "from typing import Union\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание эксперемента\n",
    "В этом примере мы применим методы визуализации для анализа классификационной модели.  \n",
    "Пусть у нас есть набор данных caltech101, для которого мы хотим получить хороший классификатор. \n",
    " \n",
    "Этот ноутбук состоит из следующих этапов:\n",
    "1. Подготовка данных\n",
    "2. Загрузка предобученной на ImageNet модели, которую мы будем дообучать\n",
    "3. Дообучение модели\n",
    "4. Анализ ошибок модели на валидации с помощью методов визуализации, рассмотренных на лекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's have a cell with global hyperparameters for the CNNs in this notebook\n",
    "\n",
    "# Path to a directory with image dataset and subfolders for training, validation and final testing\n",
    "DATA_PATH = \"../datasets\"  # PATH TO THE DATASET\n",
    "\n",
    "# Number of threads for data loader\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Image size: even though image sizes are bigger than 96, we use this to speed up training\n",
    "SIZE_H = SIZE_W = 224\n",
    "N_CHANNELS = 3\n",
    "\n",
    "# Number of classes in the dataset\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Epochs: number of passes over the training data, we use it this small to reduce training babysitting time\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# Batch size: for batch gradient descent optimization, usually selected as 2**K elements\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Images mean and std channelwise\n",
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Last layer (embeddings) size for CNN models\n",
    "EMBEDDING_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((SIZE_H, SIZE_W)),  # scaling images to fixed size\n",
    "        transforms.ToTensor(),  # converting to tensors\n",
    "        transforms.Lambda(\n",
    "            lambda x: torch.cat([x, x, x], 0) if x.shape[0] == 1 else x\n",
    "        ),  # treat gray images\n",
    "        transforms.Normalize(image_mean, image_std),  # normalize image data per-channel\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61f08f633d04f868133f036e0afbedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../datasets/caltech101/101_ObjectCategories.tar.gz to ../datasets/caltech101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d04db2ef2b459b8de39234a822ce5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../datasets/caltech101/Annotations.tar to ../datasets/caltech101\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load dataset and split it into train and val\n",
    "caltech101 = torchvision.datasets.Caltech101(\n",
    "    root=DATA_PATH, download=True, transform=transformer\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(caltech101, [7000, 1677])\n",
    "\n",
    "caltech101_unchanged = torchvision.datasets.Caltech101(root=DATA_PATH, download=True)\n",
    "torch.manual_seed(0)\n",
    "train_dataset_unchanged, val_dataset_unchanged = torch.utils.data.random_split(\n",
    "    caltech101_unchanged, [7000, 1677]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val = len(train_dataset), len(val_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Загрузка предобученной на ImageNet модели VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devel/ws.leonid/seminars/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/devel/ws.leonid/seminars/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" VGG16\n",
    "    \"\"\"\n",
    "\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Дообучение модели VGG16 на нашем датасете\n",
    "Если у вас не хватает видео памяти, попробуйте уменьшить размер батча *BATCH_SIZE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, val_loader):\n",
    "    val_accuracy = []\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        # move data to target device\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        # compute logits\n",
    "        logits = model(X_batch)\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        val_accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, opt, n_epochs):\n",
    "    \"\"\"\n",
    "    model: нейросеть для обучения,\n",
    "    train_loader, val_loader: загрузчики данных\n",
    "    loss_fn: целевая метрика (которую будем оптимизировать)\n",
    "    opt: оптимизатор (обновляет веса нейросети)\n",
    "    n_epochs: кол-во эпох, полных проходов датасета\n",
    "    \"\"\"\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True)  # enable dropout / batch_norm training behavior\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # move data to target device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # train on batch: compute loss, calc grads, perform optimizer step and zero the grads\n",
    "            opt.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = loss_fn(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            #             torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            opt.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        model.train(False)  # disable dropout / use averages for batch_norm\n",
    "        val_accuracy += compute_accuracy(model, val_loader)\n",
    "\n",
    "        # print the results for this epoch:\n",
    "        print(f\"Epoch {epoch + 1} of {n_epochs} took {time.time() - start_time:.3f}s\")\n",
    "\n",
    "        train_loss_value = np.mean(train_loss[-n_train // BATCH_SIZE :])\n",
    "        val_accuracy_value = np.mean(val_accuracy[-n_val // BATCH_SIZE :]) * 100\n",
    "\n",
    "        print(f\"  training loss (in-iteration): \\t{train_loss_value:.6f}\")\n",
    "        print(f\"  validation accuracy: \\t\\t\\t{val_accuracy_value:.2f} %\")\n",
    "\n",
    "    return train_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 took 65.559s\n",
      "  training loss (in-iteration): \t1.282204\n",
      "  validation accuracy: \t\t\t91.43 %\n",
      "Epoch 2 of 30 took 66.473s\n",
      "  training loss (in-iteration): \t0.259038\n",
      "  validation accuracy: \t\t\t92.66 %\n",
      "Epoch 3 of 30 took 66.967s\n",
      "  training loss (in-iteration): \t0.116465\n",
      "  validation accuracy: \t\t\t92.96 %\n",
      "Epoch 4 of 30 took 67.112s\n",
      "  training loss (in-iteration): \t0.071890\n",
      "  validation accuracy: \t\t\t93.25 %\n",
      "Epoch 5 of 30 took 67.167s\n",
      "  training loss (in-iteration): \t0.031698\n",
      "  validation accuracy: \t\t\t93.19 %\n",
      "Epoch 6 of 30 took 67.244s\n",
      "  training loss (in-iteration): \t0.033263\n",
      "  validation accuracy: \t\t\t93.84 %\n",
      "Epoch 7 of 30 took 67.261s\n",
      "  training loss (in-iteration): \t0.018628\n",
      "  validation accuracy: \t\t\t94.14 %\n",
      "Epoch 8 of 30 took 67.262s\n",
      "  training loss (in-iteration): \t0.020157\n",
      "  validation accuracy: \t\t\t93.49 %\n",
      "Epoch 9 of 30 took 67.373s\n",
      "  training loss (in-iteration): \t0.023079\n",
      "  validation accuracy: \t\t\t94.14 %\n",
      "Epoch 10 of 30 took 67.407s\n",
      "  training loss (in-iteration): \t0.018414\n",
      "  validation accuracy: \t\t\t92.99 %\n",
      "Epoch 11 of 30 took 67.369s\n",
      "  training loss (in-iteration): \t0.012981\n",
      "  validation accuracy: \t\t\t94.37 %\n",
      "Epoch 12 of 30 took 67.292s\n",
      "  training loss (in-iteration): \t0.008430\n",
      "  validation accuracy: \t\t\t95.26 %\n",
      "Epoch 13 of 30 took 67.320s\n",
      "  training loss (in-iteration): \t0.007474\n",
      "  validation accuracy: \t\t\t94.73 %\n",
      "Epoch 14 of 30 took 67.425s\n",
      "  training loss (in-iteration): \t0.003033\n",
      "  validation accuracy: \t\t\t94.84 %\n",
      "Epoch 15 of 30 took 67.448s\n",
      "  training loss (in-iteration): \t0.008458\n",
      "  validation accuracy: \t\t\t94.49 %\n",
      "Epoch 16 of 30 took 67.337s\n",
      "  training loss (in-iteration): \t0.010324\n",
      "  validation accuracy: \t\t\t94.73 %\n",
      "Epoch 17 of 30 took 67.465s\n",
      "  training loss (in-iteration): \t0.006362\n",
      "  validation accuracy: \t\t\t95.02 %\n",
      "Epoch 18 of 30 took 67.398s\n",
      "  training loss (in-iteration): \t0.005494\n",
      "  validation accuracy: \t\t\t94.96 %\n",
      "Epoch 19 of 30 took 67.443s\n",
      "  training loss (in-iteration): \t0.005691\n",
      "  validation accuracy: \t\t\t95.43 %\n",
      "Epoch 20 of 30 took 67.444s\n",
      "  training loss (in-iteration): \t0.002320\n",
      "  validation accuracy: \t\t\t95.02 %\n",
      "Epoch 21 of 30 took 67.329s\n",
      "  training loss (in-iteration): \t0.001659\n",
      "  validation accuracy: \t\t\t95.37 %\n",
      "Epoch 22 of 30 took 67.415s\n",
      "  training loss (in-iteration): \t0.003418\n",
      "  validation accuracy: \t\t\t94.67 %\n",
      "Epoch 23 of 30 took 67.402s\n",
      "  training loss (in-iteration): \t0.001919\n",
      "  validation accuracy: \t\t\t95.08 %\n",
      "Epoch 24 of 30 took 67.467s\n",
      "  training loss (in-iteration): \t0.001823\n",
      "  validation accuracy: \t\t\t94.67 %\n",
      "Epoch 25 of 30 took 67.393s\n",
      "  training loss (in-iteration): \t0.001140\n",
      "  validation accuracy: \t\t\t94.49 %\n",
      "Epoch 26 of 30 took 67.471s\n",
      "  training loss (in-iteration): \t0.001546\n",
      "  validation accuracy: \t\t\t94.90 %\n",
      "Epoch 27 of 30 took 67.451s\n",
      "  training loss (in-iteration): \t0.002749\n",
      "  validation accuracy: \t\t\t94.90 %\n",
      "Epoch 28 of 30 took 67.397s\n",
      "  training loss (in-iteration): \t0.002098\n",
      "  validation accuracy: \t\t\t95.26 %\n",
      "Epoch 29 of 30 took 67.380s\n",
      "  training loss (in-iteration): \t0.005273\n",
      "  validation accuracy: \t\t\t94.02 %\n",
      "Epoch 30 of 30 took 67.430s\n",
      "  training loss (in-iteration): \t0.004250\n",
      "  validation accuracy: \t\t\t94.96 %\n"
     ]
    }
   ],
   "source": [
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loss, val_accuracy = train_model(\n",
    "    model_ft, train_loader, val_loader, loss_fn, optimizer_ft, EPOCH_NUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), \"../trained_models/vgg16.pt\")\n",
    "\n",
    "# load model\n",
    "\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device)\n",
    "model_ft.load_state_dict(torch.load(\"../trained_models/vgg16.pt\"))\n",
    "model_ft.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.96099419448475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute val accuracy\n",
    "\n",
    "np.mean(compute_accuracy(model_ft, val_loader)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Анализ предсказаний нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация одного примера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../seminars/\")\n",
    "from s5_visualization.scripts.visualize_cnn import get_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devel/ws.leonid/seminars/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "image_index = 44\n",
    "image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "get_explanations(\n",
    "    model_ft,\n",
    "    image_transformed,\n",
    "    image_unchanged,\n",
    "    caltech101.categories[image_category],\n",
    "    image_category,\n",
    "    Path(\"../outputs/test.png\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1677/1677 [35:26<00:00,  1.27s/it] \n"
     ]
    }
   ],
   "source": [
    "model_ft.eval()\n",
    "out_dir = Path(\"../outputs/caltech101_vis/\")\n",
    "for image_index in tqdm(range(len(val_dataset_unchanged))):\n",
    "    image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "    true_category_name = caltech101.categories[image_category]\n",
    "    # prepare image\n",
    "    image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "    # get class scores\n",
    "    class_scores = model_ft(image_transformed)\n",
    "    class_scores = class_scores.detach().cpu().numpy()[0]\n",
    "    predicted_class = np.argmax(class_scores)\n",
    "\n",
    "    formated_image_index = str(image_index).zfill(4)\n",
    "\n",
    "    if predicted_class == image_category:\n",
    "        # right classified image. Save its visualization to foulder with class category name\n",
    "\n",
    "        save_path = out_dir / Path(\n",
    "            f\"true/{true_category_name}_id_{image_category}/{formated_image_index}.png\"\n",
    "        )\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path,\n",
    "        )\n",
    "    else:\n",
    "        # misclassified image. Save vis with respect to true and predicted classes\n",
    "        predicted_category_name = caltech101.categories[predicted_class]\n",
    "        predicted_class_score = str(round(class_scores[predicted_class], 3))\n",
    "        true_class_score = str(round(class_scores[image_category], 3))\n",
    "        save_path_predicted_vis = out_dir / Path(\n",
    "            f\"mis_class/{predicted_category_name}_id_{predicted_class}_predicted\"\n",
    "        )\n",
    "        save_path_predicted_vis /= Path(\n",
    "            f\"{formated_image_index}_predicted_category({predicted_category_name})_score_{predicted_class_score}_vis.png\"\n",
    "        )\n",
    "\n",
    "        save_path_true_vis = out_dir / Path(\n",
    "            f\"mis_class/{predicted_category_name}_id_{predicted_class}_predicted\"\n",
    "        )\n",
    "        save_path_true_vis /= Path(\n",
    "            f\"{formated_image_index}_true_category({true_category_name})_score_{true_class_score}_vis.png\"\n",
    "        )\n",
    "\n",
    "        # predicted target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            predicted_class,\n",
    "            save_path_predicted_vis,\n",
    "        )\n",
    "        # true target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path_true_vis,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbd214206844d032e7ca841ed0642b378d73189af10877ee326de88a48fbee75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
