{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контрастное обучение с SimCLR\n",
    "[SimCLR](https://arxiv.org/abs/2002.05709) (Simple Contrastive Learning Representation): self-supervised модель, которая используется для получения осмысленных представлений изображений\n",
    "\n",
    "<!-- <img src=\"../images/simclr_im1.png\" alt=\"drawing\" width=\"600\"/> -->\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive learning framework\n",
    "\n",
    "<!-- <img src=\"../images/simclr_im2.png\" alt=\"drawing\" width=\"700\"/> -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение на CIFAR-10\n",
    "Обучим модель извлечения признаков изображений на наборе данных [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). Для обучения эмбеддингов будем использовать контрастную функцию потерь.\n",
    "\n",
    "Из набора данных выбираем $N$ изображений и для каждого из них получаем 2 избражения, используя 2 случаных преобразования (кроп, изменение цвета) исходного изображения. Пару изображений, полученных от одного изображения будем называть положительной парой, иначе отрицательной. Теперь мы имеем $2N$ изображений\n",
    "\n",
    "Для каждой пары положительной пары $(i,j)$ определим функцию потерь, которая вынуждает модель выдавать близкие по метрике эмбеддинги для положительных пар, и далёкие для отрицательных.\n",
    "\n",
    "$$\n",
    "l_{i,j} = -\\log\\frac{\\exp(\\text{sim}(\\textbf{z}_i, \\textbf{z}_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(\\textbf{z}_i,\\textbf{z}_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{sim}(\\textbf{u}, \\textbf{v}) = \\textbf{u}^T\\textbf{v}/\\left\\lVert\\textbf{u}\\right\\rVert \\left\\lVert\\textbf{v}\\right\\rVert\n",
    "$$\n",
    "\n",
    "Итоговая функция потерь:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2N}\\sum_{k=1}^N[l(2k-1,2k) + l(2k, 2k - 1)]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devel/miniconda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.Tensor([[1, 2, 3],\n",
    "# [-2, 1, 2],\n",
    "# [-3, 3, 1]])\n",
    "\n",
    "# a.diagonal(dim1=-1, dim2=-2).zero_()\n",
    "# a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание модели\n",
    "\n",
    "В качестве бекбона будем использовать модификацию [ResNet-50](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "Так как разрешение изображений в наборе данных CIFAR-10, меньше чем в на наборе данных [ImageNet](https://www.image-net.org/), мы заменим первый свёрточный слой с ядром $(7\\times 7)$ и страйдом $2$, на свёрточный слой с ядром размера $(3\\times3)$ и страйдом $1$  \n",
    "Также мы удалим первый maxpolling слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_distortion(s=0.5):\n",
    "    # s is the strength of color distortion.\n",
    "    color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
    "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
    "    return color_distort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=False).to(device)\n",
    "list(resnet50.children())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modules = [\n",
    "    nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "] + list(resnet50.children())[4:-1]\n",
    "resnet50_cifar = torch.nn.Sequential(*new_modules).to(device)\n",
    "\n",
    "# summary(resnet50_cifar, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_encoder: torch.nn.Module,\n",
    "        projection_dim=128,\n",
    "        temp=0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.temp = temp\n",
    "\n",
    "        # define transforms\n",
    "        self.transformations = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(size=(32, 32)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                get_color_distortion(s=0.5),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # define base_encoder x -> h\n",
    "        self.base_encoder = base_encoder\n",
    "        self.latent_dim = 2048\n",
    "\n",
    "        # define projection head h -> z\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.projection_dim, bias=False),\n",
    "            nn.BatchNorm1d(self.projection_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_dim, self.projection_dim, bias=False),\n",
    "            nn.BatchNorm1d(self.projection_dim, affine=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Takes batch of augmented images, and computes contrastive loss\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = images[0].shape[0]\n",
    "\n",
    "        first_view, second_view = images\n",
    "\n",
    "        # compute embeddings\n",
    "        first_h = self.base_encoder(first_view)\n",
    "        second_h = self.base_encoder(second_view)\n",
    "\n",
    "        first_z = self.projection_head(torch.squeeze(first_h))\n",
    "        second_z = self.projection_head(torch.squeeze(second_h))\n",
    "\n",
    "        # normalize\n",
    "        first_z, second_z = F.normalize(first_z), F.normalize(second_z)\n",
    "\n",
    "        # compute similarities\n",
    "\n",
    "        view_cat = torch.cat([first_z, second_z], dim=0)  # 2N x d\n",
    "        s = view_cat @ view_cat.T  # 2N x 2N\n",
    "\n",
    "        s = s / self.temp\n",
    "\n",
    "        # Mask out same-sample terms\n",
    "\n",
    "        s[torch.arange(2 * batch_size), torch.arange(2 * batch_size)] = -float(\"inf\")\n",
    "\n",
    "        # compute loss\n",
    "        targets = torch.cat(\n",
    "            (\n",
    "                torch.arange(batch_size, 2 * batch_size),\n",
    "                torch.arange(0, batch_size),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        targets = targets.to(s.get_device()).long()\n",
    "\n",
    "        loss = F.cross_entropy(s, targets, reduction=\"sum\")\n",
    "\n",
    "        loss = loss / (2 * batch_size)\n",
    "\n",
    "        return loss, first_h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    if train:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(32),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return transform\n",
    "\n",
    "\n",
    "class SimCLRDataTransform(object):\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        xi = self.transform(sample)\n",
    "        xj = self.transform(sample)\n",
    "        return xi, xj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data\"\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 12\n",
    "\n",
    "N_EPOCH = 120\n",
    "LR = 1.0\n",
    "MOMENTUM = 0.9\n",
    "WD = 1e-6\n",
    "\n",
    "\n",
    "train_dset = torchvision.datasets.CIFAR10(\n",
    "    DATA_PATH,\n",
    "    train=True,\n",
    "    transform=SimCLRDataTransform(get_transform(train=True)),\n",
    "    download=True,\n",
    ")\n",
    "test_dset = torchvision.datasets.CIFAR10(\n",
    "    DATA_PATH,\n",
    "    train=False,\n",
    "    transform=SimCLRDataTransform(get_transform(train=False)),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=12,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    test_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=12,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lars import LARS\n",
    "\n",
    "model = SimCLR(base_encoder=resnet50_cifar).to(device)\n",
    "optimizer = LARS(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WD,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/s12_self-supervised/notebooks/lars.py:89: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  buf.mul_(momentum).add_(actual_lr, d_p + weight_decay * p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.55462646484375\n",
      "5.543821334838867\n",
      "5.524416923522949\n",
      "5.528378963470459\n",
      "5.5681352615356445\n",
      "5.561792373657227\n",
      "5.501705169677734\n",
      "5.514983177185059\n",
      "5.539113998413086\n",
      "5.530882835388184\n",
      "5.479883193969727\n",
      "5.628942966461182\n",
      "5.50087308883667\n",
      "5.4649834632873535\n",
      "5.430487155914307\n",
      "5.562296390533447\n",
      "5.500844955444336\n",
      "5.498812198638916\n",
      "5.427420139312744\n",
      "5.500797748565674\n",
      "5.530545234680176\n",
      "5.453332901000977\n",
      "5.541370391845703\n",
      "5.436233997344971\n",
      "5.358013153076172\n",
      "5.432920455932617\n",
      "5.321462631225586\n",
      "5.326171875\n",
      "5.42201042175293\n",
      "5.273672580718994\n",
      "5.380896091461182\n",
      "5.31166934967041\n",
      "5.2001519203186035\n",
      "5.384452819824219\n",
      "5.320104598999023\n",
      "5.332322597503662\n",
      "5.18975305557251\n",
      "5.296821594238281\n",
      "5.241923809051514\n",
      "5.303590774536133\n",
      "5.251042366027832\n",
      "5.119968891143799\n",
      "5.110445499420166\n",
      "5.220890045166016\n",
      "5.111337184906006\n",
      "5.008529186248779\n",
      "5.035027027130127\n",
      "5.013930320739746\n",
      "4.998182773590088\n",
      "5.097278118133545\n",
      "5.026236534118652\n",
      "5.106514930725098\n",
      "5.019130229949951\n",
      "5.0167436599731445\n",
      "5.000108242034912\n",
      "5.056652545928955\n",
      "5.057281494140625\n",
      "5.094603061676025\n",
      "5.055342674255371\n",
      "5.00149393081665\n",
      "5.03905725479126\n",
      "4.98867130279541\n",
      "5.0493974685668945\n",
      "4.986694812774658\n",
      "4.990468502044678\n",
      "5.10447359085083\n",
      "4.947338104248047\n",
      "4.934269905090332\n",
      "5.039461135864258\n",
      "4.881259918212891\n",
      "4.9726691246032715\n",
      "4.952210903167725\n",
      "4.875877380371094\n",
      "4.938796043395996\n",
      "4.899288177490234\n",
      "4.882192611694336\n",
      "4.835636615753174\n",
      "4.920194625854492\n",
      "4.787553787231445\n",
      "4.897234916687012\n",
      "4.875338077545166\n",
      "5.03891134262085\n",
      "4.851573944091797\n",
      "4.810748100280762\n",
      "4.90226411819458\n",
      "4.966373920440674\n",
      "4.863829135894775\n",
      "4.838898181915283\n",
      "4.860566139221191\n",
      "4.829524517059326\n",
      "4.890201568603516\n",
      "4.959717750549316\n",
      "4.850124835968018\n",
      "4.763332366943359\n",
      "4.677942752838135\n",
      "4.824652194976807\n",
      "4.859520435333252\n",
      "4.8035664558410645\n",
      "4.924598693847656\n",
      "4.890383243560791\n",
      "4.835468769073486\n",
      "4.78414249420166\n",
      "4.8706278800964355\n",
      "4.841464996337891\n",
      "4.821828842163086\n",
      "4.760439395904541\n",
      "4.789553165435791\n",
      "4.768842697143555\n",
      "4.811045169830322\n",
      "4.756025791168213\n",
      "4.813858985900879\n",
      "4.913490295410156\n",
      "4.729172229766846\n",
      "4.9346795082092285\n",
      "4.745301723480225\n",
      "4.895618438720703\n",
      "4.793416976928711\n",
      "4.8279314041137695\n",
      "4.872037410736084\n",
      "4.750505447387695\n",
      "4.879001617431641\n",
      "4.784431457519531\n",
      "4.89833402633667\n",
      "4.930159568786621\n",
      "4.772449016571045\n",
      "4.708403587341309\n",
      "4.843261241912842\n",
      "4.752895832061768\n",
      "4.749272346496582\n",
      "4.737730979919434\n",
      "4.826548099517822\n",
      "4.857122898101807\n",
      "4.781987190246582\n",
      "4.867844104766846\n",
      "4.831125736236572\n",
      "4.872317790985107\n",
      "4.810176849365234\n",
      "4.852116107940674\n",
      "4.705766677856445\n",
      "4.8113789558410645\n",
      "4.804233551025391\n",
      "4.853773593902588\n",
      "4.8118486404418945\n",
      "4.727414131164551\n",
      "4.788497447967529\n",
      "4.765000820159912\n",
      "4.781349182128906\n",
      "4.876473426818848\n",
      "4.906101703643799\n",
      "4.717653274536133\n",
      "4.865838527679443\n",
      "4.856237411499023\n",
      "4.808793544769287\n",
      "4.906105995178223\n",
      "4.80914831161499\n",
      "4.754057884216309\n",
      "4.639944076538086\n",
      "4.750572204589844\n",
      "4.681854724884033\n",
      "4.740448474884033\n",
      "4.730795860290527\n",
      "4.815016269683838\n",
      "4.656215667724609\n",
      "4.767215728759766\n",
      "4.834524631500244\n",
      "4.794280052185059\n",
      "4.834668159484863\n",
      "4.74554967880249\n",
      "4.737417221069336\n",
      "4.646141052246094\n",
      "4.797469139099121\n",
      "4.697612762451172\n",
      "4.775004863739014\n",
      "4.714744567871094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 10\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/app/s12_self-supervised/notebooks/lars.py:89\u001b[0m, in \u001b[0;36mLARS.step\u001b[0;34m(self, epoch, closure)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m             buf \u001b[39m=\u001b[39m param_state[\u001b[39m\"\u001b[39m\u001b[39mmomentum_buffer\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 89\u001b[0m         buf\u001b[39m.\u001b[39;49mmul_(momentum)\u001b[39m.\u001b[39;49madd_(actual_lr, d_p \u001b[39m+\u001b[39;49m weight_decay \u001b[39m*\u001b[39;49m p\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m     90\u001b[0m         p\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39madd_(\u001b[39m-\u001b[39mbuf)\n\u001b[1;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for _ in range(N_EPOCH):\n",
    "        for images, _ in train_loader:\n",
    "            images = [x.to(device) for x in images]\n",
    "            loss, hs = model(images)\n",
    "            hs = hs.detach()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbd214206844d032e7ca841ed0642b378d73189af10877ee326de88a48fbee75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
