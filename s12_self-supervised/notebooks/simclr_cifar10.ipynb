{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контрастное обучение с SimCLR\n",
    "[SimCLR](https://arxiv.org/abs/2002.05709) (Simple Contrastive Learning Representation): self-supervised модель, которая используется для получения осмысленных представлений изображений\n",
    "\n",
    "<!-- <img src=\"../images/simclr_im1.png\" alt=\"drawing\" width=\"600\"/> -->\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive learning framework\n",
    "\n",
    "<!-- <img src=\"../images/simclr_im2.png\" alt=\"drawing\" width=\"700\"/> -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение на CIFAR-10\n",
    "Обучим модель извлечения признаков изображений на наборе данных [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). Для обучения эмбеддингов будем использовать контрастную функцию потерь.\n",
    "\n",
    "Из набора данных выбираем $N$ изображений и для каждого из них получаем 2 избражения, используя 2 случаных преобразования (кроп, изменение цвета) исходного изображения. Пару изображений, полученных от одного изображения будем называть положительной парой, иначе отрицательной. Теперь мы имеем $2N$ изображений\n",
    "\n",
    "Для каждой пары положительной пары $(i,j)$ определим функцию потерь, которая вынуждает модель выдавать близкие по метрике эмбеддинги для положительных пар, и далёкие для отрицательных.\n",
    "\n",
    "$$\n",
    "l_{i,j} = -\\log\\frac{\\exp(\\text{sim}(\\textbf{z}_i, \\textbf{z}_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(\\textbf{z}_i,\\textbf{z}_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{sim}(\\textbf{u}, \\textbf{v}) = \\textbf{u}^T\\textbf{v}/\\left\\lVert\\textbf{u}\\right\\rVert \\left\\lVert\\textbf{v}\\right\\rVert\n",
    "$$\n",
    "\n",
    "Итоговая функция потерь:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2N}\\sum_{k=1}^N[l(2k-1,2k) + l(2k, 2k - 1)]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devel/miniconda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание модели\n",
    "\n",
    "В качестве бекбона будем использовать модификацию [ResNet-50](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "Так как разрешение изображений в наборе данных CIFAR-10, меньше чем в на наборе данных [ImageNet](https://www.image-net.org/), мы заменим первый свёрточный слой с ядром $(7\\times 7)$ и страйдом $2$, на свёрточный слой с ядром размера $(3\\times3)$ и страйдом $1$  \n",
    "Также мы удалим первый maxpolling слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=False).to(device)\n",
    "list(resnet50.children())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modules = [\n",
    "    nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "] + list(resnet50.children())[4:-1]\n",
    "resnet50_cifar = torch.nn.Sequential(*new_modules).to(device)\n",
    "\n",
    "# summary(resnet50_cifar, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_encoder: torch.nn.Module,\n",
    "        projection_dim=128,\n",
    "        temp=0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.temp = temp\n",
    "\n",
    "        # define base_encoder x -> h\n",
    "        self.base_encoder = base_encoder\n",
    "        self.latent_dim = 2048\n",
    "\n",
    "        # define projection head h -> z\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.projection_dim, bias=False),\n",
    "            nn.BatchNorm1d(self.projection_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.projection_dim, self.projection_dim, bias=False),\n",
    "            nn.BatchNorm1d(self.projection_dim, affine=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Takes batch of augmented images, and computes contrastive loss\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = images[0].shape[0]\n",
    "\n",
    "        first_view, second_view = images\n",
    "\n",
    "        # compute embeddings\n",
    "        first_h = torch.squeeze(self.base_encoder(first_view))\n",
    "        second_h = torch.squeeze(self.base_encoder(second_view))\n",
    "\n",
    "        first_z = self.projection_head(first_h)\n",
    "        second_z = self.projection_head(second_h)\n",
    "\n",
    "        # normalize\n",
    "        first_z, second_z = F.normalize(first_z), F.normalize(second_z)\n",
    "\n",
    "        # compute similarities\n",
    "\n",
    "        view_cat = torch.cat([first_z, second_z], dim=0)  # 2N x d\n",
    "        s = view_cat @ view_cat.T  # 2N x 2N\n",
    "\n",
    "        s = s / self.temp\n",
    "\n",
    "        # Mask out same-sample terms\n",
    "\n",
    "        s[torch.arange(2 * batch_size), torch.arange(2 * batch_size)] = -float(\"inf\")\n",
    "\n",
    "        # compute loss\n",
    "        targets = torch.cat(\n",
    "            (\n",
    "                torch.arange(batch_size, 2 * batch_size),\n",
    "                torch.arange(0, batch_size),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        targets = targets.to(s.get_device()).long()\n",
    "\n",
    "        loss = F.cross_entropy(s, targets, reduction=\"sum\")\n",
    "\n",
    "        loss = loss / (2 * batch_size)\n",
    "\n",
    "        return loss, first_h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train, sim_clr_trans=True):\n",
    "    if train:\n",
    "        if sim_clr_trans:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomResizedCrop(32),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomApply(\n",
    "                        [transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8\n",
    "                    ),\n",
    "                    transforms.RandomGrayscale(p=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((32, 32)),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return transform\n",
    "\n",
    "\n",
    "class SimCLRDataTransform(object):\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        xi = self.transform(sample)\n",
    "        xj = self.transform(sample)\n",
    "        return xi, xj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data\"\n",
    "OUT_PATH = Path(\"../outputs/cifar_10_ssl_v2\")\n",
    "OUT_PATH.mkdir(exist_ok=True)\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 12\n",
    "\n",
    "N_EPOCH = 1000\n",
    "LR = 1.0\n",
    "MOMENTUM = 0.9\n",
    "WD = 1e-6\n",
    "\n",
    "\n",
    "train_dset = torchvision.datasets.CIFAR10(\n",
    "    DATA_PATH,\n",
    "    train=True,\n",
    "    transform=SimCLRDataTransform(get_transform(train=True)),\n",
    "    download=True,\n",
    ")\n",
    "test_dset = torchvision.datasets.CIFAR10(\n",
    "    DATA_PATH,\n",
    "    train=False,\n",
    "    transform=SimCLRDataTransform(get_transform(train=False)),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=12,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    test_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=12,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение\n",
    "Всместе с Self-supervised моделью будем обучать линейную модель, для подсчёта точности предсказаний линейной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lars import LARS\n",
    "\n",
    "\n",
    "# self-supervised model\n",
    "model = SimCLR(base_encoder=resnet50_cifar).to(device)\n",
    "optimizer = LARS(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WD,\n",
    ")\n",
    "\n",
    "# linear model\n",
    "linear_classifier = nn.Sequential(nn.Linear(model.latent_dim, 10)).to(device)\n",
    "optimizer_linear = torch.optim.SGD(\n",
    "    linear_classifier.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    nesterov=True,\n",
    ")\n",
    "\n",
    "# define schdulers\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, N_EPOCH, 0, -1)\n",
    "scheduler_linear = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_linear, N_EPOCH, 0, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AverageMeter, ProgressMeter, accuracy\n",
    "import time\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_loader, model, linear_classifier, optimizer, optimizer_linear, epoch, device\n",
    "):\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    top1 = AverageMeter(\"LinearAcc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"LinearAcc@5\", \":6.2f\")\n",
    "    avg_meters = {k: AverageMeter(k, fmt) for k, fmt in zip([\"Loss\"], [\":.4e\"])}\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, top1, top5] + list(avg_meters.values()),\n",
    "        prefix=\"Epoch: [{}]\".format(epoch),\n",
    "    )\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    linear_classifier.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        batch_size = images[0].shape[0]\n",
    "\n",
    "        images = [x.to(device) for x in images]\n",
    "        target = target.to(device)\n",
    "\n",
    "        loss, hs = model(images)\n",
    "        hs = hs.detach()\n",
    "\n",
    "        avg_meters[\"Loss\"].update(loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and optimizer step for ssl task\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute gradient and optimizer step for classifier\n",
    "        logits = linear_classifier(hs)\n",
    "        loss_linear = F.cross_entropy(logits, target)\n",
    "\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], batch_size)\n",
    "        top5.update(acc5[0], batch_size)\n",
    "\n",
    "        optimizer_linear.zero_grad()\n",
    "        loss_linear.backward()\n",
    "        optimizer_linear.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, linear_classifier, device):\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    top1 = AverageMeter(\"LinearAcc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"LinearAcc@5\", \":6.2f\")\n",
    "    avg_meters = {k: AverageMeter(k, fmt) for k, fmt in zip([\"Loss\"], [\":.4e\"])}\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, data_time, top1, top5] + list(avg_meters.values()),\n",
    "        prefix=\"Test: \",\n",
    "    )\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    linear_classifier.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            # compute and measure loss\n",
    "            batch_size = images[0].shape[0]\n",
    "\n",
    "            images = [x.to(device) for x in images]\n",
    "            target = target.to(device)\n",
    "\n",
    "            loss, hs = model(images)\n",
    "\n",
    "            avg_meters[\"Loss\"].update(loss.item(), batch_size)\n",
    "\n",
    "            logits = linear_classifier(hs)\n",
    "            acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "            top1.update(acc1[0], batch_size)\n",
    "            top5.update(acc5[0], batch_size)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "    data = torch.FloatTensor(\n",
    "        [avg_meters[\"Loss\"].avg, top1.avg, top5.avg]\n",
    "        + [v.avg for v in avg_meters.values()]\n",
    "    )\n",
    "\n",
    "    print_str = f\" * LinearAcc@1 {data[1]:.3f} LinearAcc@5 {data[2]:.3f}\"\n",
    "    for i, (k, v) in enumerate(avg_meters.items()):\n",
    "        print_str += f\" {k} {data[i+3]:.3f}\"\n",
    "    print(print_str)\n",
    "\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, out_path=OUT_PATH, filename=\"checkpoint.pth.tar\"):\n",
    "    filename = out_path / filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, out_path / \"model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss = float(\"inf\")\n",
    "# best_acc = 0.0\n",
    "# model.train()\n",
    "# for epoch in range(N_EPOCH):\n",
    "#     # train\n",
    "#     train(\n",
    "#             train_loader,\n",
    "#             model,\n",
    "#             linear_classifier,\n",
    "#             optimizer,\n",
    "#             optimizer_linear,\n",
    "#             epoch,\n",
    "#             device\n",
    "#         )\n",
    "\n",
    "#     # validate\n",
    "#     val_loss, val_acc = validate(val_loader, model, linear_classifier, device)\n",
    "\n",
    "#     # update scheduler\n",
    "#     scheduler.step()\n",
    "#     scheduler_linear.step()\n",
    "\n",
    "#     # save checkpoint\n",
    "#     is_best = val_loss < best_loss\n",
    "#     best_loss = min(val_loss, best_loss)\n",
    "#     save_checkpoint(\n",
    "#         {\n",
    "#             \"epoch\": epoch + 1,\n",
    "#             \"state_dict\": model.state_dict(),\n",
    "#             \"optimizer\": optimizer.state_dict(),\n",
    "#             \"scheduler\": scheduler.state_dict(),\n",
    "#             \"state_dict_linear\": linear_classifier.state_dict(),\n",
    "#             \"optimizer_linear\": optimizer_linear.state_dict(),\n",
    "#             \"schedular_linear\": scheduler_linear.state_dict(),\n",
    "#             \"best_loss\": best_loss,\n",
    "#             \"best_acc\": val_acc,\n",
    "#         },\n",
    "#         is_best,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(out_path):\n",
    "    ckpt_pth = out_path / \"model_best.pth.tar\"\n",
    "    ckpt = torch.load(ckpt_pth, map_location=\"cpu\")\n",
    "\n",
    "    new_modules = [\n",
    "        nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace=True),\n",
    "    ] + list(resnet50.children())[4:-1]\n",
    "    resnet50_cifar = torch.nn.Sequential(*new_modules).to(device)\n",
    "\n",
    "    model = SimCLR(base_encoder=resnet50_cifar).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    linear_classifier = nn.Sequential(nn.Linear(model.latent_dim, 10)).to(device)\n",
    "    linear_classifier.load_state_dict(ckpt[\"state_dict_linear\"])\n",
    "\n",
    "    linear_classifier.cuda()\n",
    "    linear_classifier.eval()\n",
    "\n",
    "    return model, linear_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      "Top 1 Accuracy: 73.01, Top 5 Accuracy: 92.58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluate_classifier\n",
    "\n",
    "# model, linear_classifier = load_model(Path(\"../outputs/cifar_10_ssl\"))\n",
    "# test_acc1, test_acc5 = evaluate_classifier(model, linear_classifier, val_loader, device)\n",
    "# print(\"Test Set\")\n",
    "# print(f\"Top 1 Accuracy: {test_acc1}, Top 5 Accuracy: {test_acc5}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение с небольшим числом лейблов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar10_classifier(\n",
    "    train_loader, model, linear_classifier, optimizer, epoch, device, finetune_extractor\n",
    "):\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    top1 = AverageMeter(\"LinearAcc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"LinearAcc@5\", \":6.2f\")\n",
    "    avg_meters = {k: AverageMeter(k, fmt) for k, fmt in zip([\"Loss\"], [\":.4e\"])}\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, top1, top5] + list(avg_meters.values()),\n",
    "        prefix=\"Epoch: [{}]\".format(epoch),\n",
    "    )\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    linear_classifier.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        hs = torch.squeeze(model(images))\n",
    "\n",
    "        if not finetune_extractor:\n",
    "            hs = hs.detach()\n",
    "        # compute gradient and optimizer step for classifier\n",
    "        logits = linear_classifier(hs)\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        avg_meters[\"Loss\"].update(loss.item(), batch_size)\n",
    "\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], batch_size)\n",
    "        top5.update(acc5[0], batch_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cifar10_classifier(val_loader, model, linear_classifier, device):\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    top1 = AverageMeter(\"LinearAcc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"LinearAcc@5\", \":6.2f\")\n",
    "    avg_meters = {k: AverageMeter(k, fmt) for k, fmt in zip([\"Loss\"], [\":.4e\"])}\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, data_time, top1, top5] + list(avg_meters.values()),\n",
    "        prefix=\"Test: \",\n",
    "    )\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    linear_classifier.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            # compute and measure loss\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            hs = torch.squeeze(model(images))\n",
    "\n",
    "            logits = linear_classifier(hs)\n",
    "            acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "            top1.update(acc1[0], batch_size)\n",
    "            top5.update(acc5[0], batch_size)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "    data = torch.FloatTensor(\n",
    "        [avg_meters[\"Loss\"].avg, top1.avg, top5.avg]\n",
    "        + [v.avg for v in avg_meters.values()]\n",
    "    )\n",
    "\n",
    "    print_str = f\" * LinearAcc@1 {data[1]:.3f} LinearAcc@5 {data[2]:.3f}\"\n",
    "    print(print_str)\n",
    "\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(\n",
    "    run_name, train_loader, val_loader, feature_extractor, n_epoch, finetune_extractor\n",
    "):\n",
    "    out_path = Path(\"../outputs\") / run_name\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "    best_loss = float(\"inf\")\n",
    "    best_acc = 0.0\n",
    "\n",
    "    linear_classifier = nn.Sequential(nn.Linear(2048, 10)).to(device)\n",
    "    if finetune_extractor:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            list(feature_extractor.parameters()) + list(linear_classifier.parameters()),\n",
    "            lr=0.001,\n",
    "            momentum=0.9,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            linear_classifier.parameters(),\n",
    "            lr=0.001,\n",
    "            momentum=0.9,\n",
    "        )\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        # train\n",
    "        train_cifar10_classifier(\n",
    "            train_loader,\n",
    "            feature_extractor,\n",
    "            linear_classifier,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            device,\n",
    "            finetune_extractor,\n",
    "        )\n",
    "\n",
    "        # validate\n",
    "        val_loss, val_acc = validate_cifar10_classifier(\n",
    "            val_loader, feature_extractor, linear_classifier, device\n",
    "        )\n",
    "\n",
    "        # update scheduler\n",
    "        # scheduler.step()\n",
    "\n",
    "        # save checkpoint\n",
    "        is_best = val_loss < best_loss\n",
    "        best_loss = min(val_loss, best_loss)\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"state_dict_linear\": linear_classifier.state_dict(),\n",
    "                \"best_loss\": best_loss,\n",
    "                \"best_acc\": val_acc,\n",
    "            },\n",
    "            is_best,\n",
    "            out_path=out_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_dset = torchvision.datasets.CIFAR10(\n",
    "    DATA_PATH,\n",
    "    train=True,\n",
    "    transform=get_transform(train=True, sim_clr_trans=False),\n",
    "    download=True,\n",
    ")\n",
    "test_dset = torchvision.datasets.CIFAR10(\n",
    "    DATA_PATH,\n",
    "    train=False,\n",
    "    transform=get_transform(train=False),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=12,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    test_dset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=12,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# class_to_instances = {class_id: [] for class_id in range(10)}\n",
    "\n",
    "# for i in range(len(train_dset)):\n",
    "#     class_to_instances[train_dset[i][1]].append(i)\n",
    "\n",
    "\n",
    "# # Save\n",
    "# np.save('class_to_instances.npy', class_to_instances) \n",
    "\n",
    "# Load\n",
    "class_to_instances = np.load('class_to_instances.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_portion_of_train_set(train_dset, fraction, class_to_instances, batch_size):\n",
    "    idx = []\n",
    "    for class_id, instances in class_to_instances.items():\n",
    "        idx += instances[: int(len(instances) * fraction)]\n",
    "\n",
    "    train_dset_fraction = torch.utils.data.Subset(train_dset, idx)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dset_fraction,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=12,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=False).to(device)\n",
    "new_modules = [\n",
    "    nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "] + list(resnet50.children())[4:-1]\n",
    "resnet50_cifar = torch.nn.Sequential(*new_modules).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = take_portion_of_train_set(train_dset, 1, class_to_instances, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ssl, linear_classifier = load_model(Path(\"../outputs/cifar_10_ssl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  0/390]\tTime  0.848 ( 0.848)\tData  0.751 ( 0.751)\tLinearAcc@1   0.00 (  0.00)\tLinearAcc@5  24.22 ( 24.22)\tLoss 2.4122e+00 (2.4122e+00)\n",
      "Epoch: [0][ 10/390]\tTime  0.089 ( 0.160)\tData  0.000 ( 0.070)\tLinearAcc@1 100.00 ( 88.92)\tLinearAcc@5 100.00 ( 93.04)\tLoss 4.7759e-04 (5.6313e-01)\n",
      "Epoch: [0][ 20/390]\tTime  0.092 ( 0.127)\tData  0.004 ( 0.037)\tLinearAcc@1 100.00 ( 94.20)\tLinearAcc@5 100.00 ( 96.35)\tLoss 6.2612e-06 (2.9501e-01)\n",
      "Epoch: [0][ 30/390]\tTime  0.090 ( 0.115)\tData  0.002 ( 0.026)\tLinearAcc@1 100.00 ( 96.07)\tLinearAcc@5 100.00 ( 97.53)\tLoss 1.4557e-06 (1.9985e-01)\n",
      "Epoch: [0][ 40/390]\tTime  0.090 ( 0.109)\tData  0.002 ( 0.020)\tLinearAcc@1   0.00 ( 92.30)\tLinearAcc@5 100.00 ( 97.50)\tLoss 1.6423e+01 (9.4961e-01)\n",
      "Epoch: [0][ 50/390]\tTime  0.090 ( 0.105)\tData  0.002 ( 0.016)\tLinearAcc@1 100.00 ( 85.97)\tLinearAcc@5 100.00 ( 97.99)\tLoss 1.1828e-07 (1.5092e+00)\n",
      "Epoch: [0][ 60/390]\tTime  0.091 ( 0.103)\tData  0.002 ( 0.014)\tLinearAcc@1 100.00 ( 88.27)\tLinearAcc@5 100.00 ( 98.32)\tLoss 0.0000e+00 (1.2618e+00)\n",
      "Epoch: [0][ 70/390]\tTime  0.090 ( 0.101)\tData  0.002 ( 0.012)\tLinearAcc@1 100.00 ( 89.92)\tLinearAcc@5 100.00 ( 98.56)\tLoss 0.0000e+00 (1.0841e+00)\n",
      "Epoch: [0][ 80/390]\tTime  0.090 ( 0.100)\tData  0.002 ( 0.011)\tLinearAcc@1   0.00 ( 87.62)\tLinearAcc@5 100.00 ( 97.65)\tLoss 2.9907e+01 (2.0636e+00)\n",
      "Epoch: [0][ 90/390]\tTime  0.091 ( 0.099)\tData  0.002 ( 0.010)\tLinearAcc@1 100.00 ( 82.38)\tLinearAcc@5 100.00 ( 97.91)\tLoss 4.7497e-08 (2.8909e+00)\n",
      "Epoch: [0][100/390]\tTime  0.090 ( 0.098)\tData  0.001 ( 0.009)\tLinearAcc@1 100.00 ( 84.13)\tLinearAcc@5 100.00 ( 98.11)\tLoss 0.0000e+00 (2.6046e+00)\n",
      "Epoch: [0][110/390]\tTime  0.091 ( 0.097)\tData  0.002 ( 0.008)\tLinearAcc@1 100.00 ( 85.56)\tLinearAcc@5 100.00 ( 98.28)\tLoss 0.0000e+00 (2.3700e+00)\n",
      "Epoch: [0][120/390]\tTime  0.090 ( 0.097)\tData  0.001 ( 0.008)\tLinearAcc@1   0.00 ( 83.60)\tLinearAcc@5 100.00 ( 98.37)\tLoss 4.1053e+01 (3.5586e+00)\n",
      "Epoch: [0][130/390]\tTime  0.091 ( 0.096)\tData  0.002 ( 0.007)\tLinearAcc@1 100.00 ( 79.51)\tLinearAcc@5 100.00 ( 98.49)\tLoss 7.0501e-07 (4.4588e+00)\n",
      "Epoch: [0][140/390]\tTime  0.091 ( 0.096)\tData  0.002 ( 0.007)\tLinearAcc@1 100.00 ( 80.96)\tLinearAcc@5 100.00 ( 98.60)\tLoss 0.0000e+00 (4.1426e+00)\n",
      "Epoch: [0][150/390]\tTime  0.090 ( 0.095)\tData  0.002 ( 0.007)\tLinearAcc@1 100.00 ( 82.22)\tLinearAcc@5 100.00 ( 98.69)\tLoss 0.0000e+00 (3.8682e+00)\n",
      "Epoch: [0][160/390]\tTime  0.091 ( 0.095)\tData  0.002 ( 0.006)\tLinearAcc@1   0.00 ( 80.38)\tLinearAcc@5 100.00 ( 98.73)\tLoss 4.8224e+01 (5.1960e+00)\n",
      "Epoch: [0][170/390]\tTime  0.091 ( 0.095)\tData  0.002 ( 0.006)\tLinearAcc@1 100.00 ( 77.27)\tLinearAcc@5 100.00 ( 98.81)\tLoss 3.8991e-06 (6.0014e+00)\n",
      "Epoch: [0][180/390]\tTime  0.090 ( 0.095)\tData  0.002 ( 0.006)\tLinearAcc@1 100.00 ( 78.53)\tLinearAcc@5 100.00 ( 98.87)\tLoss 0.0000e+00 (5.6698e+00)\n",
      "Epoch: [0][190/390]\tTime  0.090 ( 0.094)\tData  0.002 ( 0.006)\tLinearAcc@1 100.00 ( 79.65)\tLinearAcc@5 100.00 ( 98.93)\tLoss 0.0000e+00 (5.3729e+00)\n",
      "Epoch: [0][200/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1   0.00 ( 77.83)\tLinearAcc@5 100.00 ( 98.99)\tLoss 5.1867e+01 (6.7786e+00)\n",
      "Epoch: [0][210/390]\tTime  0.090 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 75.32)\tLinearAcc@5 100.00 ( 99.03)\tLoss 2.7147e-06 (7.4097e+00)\n",
      "Epoch: [0][220/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 76.44)\tLinearAcc@5 100.00 ( 99.08)\tLoss 0.0000e+00 (7.0744e+00)\n",
      "Epoch: [0][230/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 77.46)\tLinearAcc@5 100.00 ( 99.12)\tLoss 0.0000e+00 (6.7682e+00)\n",
      "Epoch: [0][240/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1   0.00 ( 75.64)\tLinearAcc@5 100.00 ( 99.15)\tLoss 5.2820e+01 (8.2095e+00)\n",
      "Epoch: [0][250/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 73.82)\tLinearAcc@5 100.00 ( 99.19)\tLoss 3.2783e-07 (8.6567e+00)\n",
      "Epoch: [0][260/390]\tTime  0.090 ( 0.093)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 74.83)\tLinearAcc@5 100.00 ( 99.22)\tLoss 0.0000e+00 (8.3250e+00)\n",
      "Epoch: [0][270/390]\tTime  0.092 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 75.76)\tLinearAcc@5 100.00 ( 99.25)\tLoss 0.0000e+00 (8.0178e+00)\n",
      "Epoch: [0][280/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1   0.00 ( 73.93)\tLinearAcc@5 100.00 ( 99.27)\tLoss 5.0357e+01 (9.4308e+00)\n",
      "Epoch: [0][290/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 72.42)\tLinearAcc@5 100.00 ( 99.30)\tLoss 2.7940e-09 (9.6740e+00)\n",
      "Epoch: [0][300/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 73.33)\tLinearAcc@5 100.00 ( 99.32)\tLoss 0.0000e+00 (9.3526e+00)\n",
      "Epoch: [0][310/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 74.19)\tLinearAcc@5 100.00 ( 99.34)\tLoss 0.0000e+00 (9.0519e+00)\n",
      "Epoch: [0][320/390]\tTime  0.090 ( 0.093)\tData  0.001 ( 0.004)\tLinearAcc@1   0.00 ( 72.35)\tLinearAcc@5 100.00 ( 99.36)\tLoss 4.7697e+01 (1.0461e+01)\n",
      "Epoch: [0][330/390]\tTime  0.091 ( 0.093)\tData  0.001 ( 0.004)\tLinearAcc@1 100.00 ( 71.37)\tLinearAcc@5 100.00 ( 99.38)\tLoss 2.7940e-09 (1.0573e+01)\n",
      "Epoch: [0][340/390]\tTime  0.092 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 72.21)\tLinearAcc@5 100.00 ( 99.40)\tLoss 0.0000e+00 (1.0263e+01)\n",
      "Epoch: [0][350/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 73.00)\tLinearAcc@5 100.00 ( 99.42)\tLoss 0.0000e+00 (9.9706e+00)\n",
      "Epoch: [0][360/390]\tTime  0.090 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1   0.00 ( 71.13)\tLinearAcc@5 100.00 ( 99.44)\tLoss 4.2999e+01 (1.1331e+01)\n",
      "Epoch: [0][370/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 70.57)\tLinearAcc@5 100.00 ( 99.45)\tLoss 0.0000e+00 (1.1316e+01)\n",
      "Epoch: [0][380/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 71.34)\tLinearAcc@5 100.00 ( 99.46)\tLoss 0.0000e+00 (1.1019e+01)\n",
      "Test: [ 0/78]\tTime  0.725 ( 0.725)\tData  0.000 ( 0.000)\tLinearAcc@1   8.59 (  8.59)\tLinearAcc@5  53.91 ( 53.91)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [10/78]\tTime  0.088 ( 0.145)\tData  0.000 ( 0.000)\tLinearAcc@1  13.28 ( 10.51)\tLinearAcc@5  52.34 ( 50.99)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [20/78]\tTime  0.086 ( 0.117)\tData  0.000 ( 0.000)\tLinearAcc@1   3.91 ( 10.01)\tLinearAcc@5  44.53 ( 50.30)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [30/78]\tTime  0.086 ( 0.107)\tData  0.000 ( 0.000)\tLinearAcc@1  11.72 ( 10.31)\tLinearAcc@5  55.47 ( 50.03)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [40/78]\tTime  0.086 ( 0.102)\tData  0.000 ( 0.000)\tLinearAcc@1   8.59 ( 10.21)\tLinearAcc@5  42.19 ( 49.79)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [50/78]\tTime  0.087 ( 0.099)\tData  0.000 ( 0.000)\tLinearAcc@1   6.25 (  9.96)\tLinearAcc@5  47.66 ( 49.85)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [60/78]\tTime  0.088 ( 0.097)\tData  0.000 ( 0.000)\tLinearAcc@1  12.50 (  9.89)\tLinearAcc@5  50.78 ( 49.73)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [70/78]\tTime  0.087 ( 0.096)\tData  0.000 ( 0.000)\tLinearAcc@1  11.72 (  9.87)\tLinearAcc@5  50.00 ( 49.78)\tLoss 0.0000e+00 (0.0000e+00)\n",
      " * LinearAcc@1 10.016 LinearAcc@5 49.990\n",
      "Epoch: [1][  0/390]\tTime  0.847 ( 0.847)\tData  0.756 ( 0.756)\tLinearAcc@1   0.00 (  0.00)\tLinearAcc@5   0.00 (  0.00)\tLoss 9.0109e+01 (9.0109e+01)\n",
      "Epoch: [1][ 10/390]\tTime  0.089 ( 0.161)\tData  0.000 ( 0.071)\tLinearAcc@1   0.00 (  0.00)\tLinearAcc@5 100.00 ( 35.87)\tLoss 4.7605e+01 (7.4013e+01)\n",
      "Epoch: [1][ 20/390]\tTime  0.091 ( 0.127)\tData  0.002 ( 0.038)\tLinearAcc@1 100.00 ( 23.81)\tLinearAcc@5 100.00 ( 66.41)\tLoss 0.0000e+00 (4.4585e+01)\n",
      "Epoch: [1][ 30/390]\tTime  0.091 ( 0.115)\tData  0.002 ( 0.026)\tLinearAcc@1 100.00 ( 48.39)\tLinearAcc@5 100.00 ( 77.24)\tLoss 0.0000e+00 (3.0203e+01)\n",
      "Epoch: [1][ 40/390]\tTime  0.091 ( 0.109)\tData  0.002 ( 0.020)\tLinearAcc@1   0.00 ( 56.25)\tLinearAcc@5   0.00 ( 78.07)\tLoss 8.3029e+01 (2.6776e+01)\n",
      "Epoch: [1][ 50/390]\tTime  0.091 ( 0.106)\tData  0.002 ( 0.016)\tLinearAcc@1   0.00 ( 45.22)\tLinearAcc@5 100.00 ( 72.56)\tLoss 3.4565e+01 (3.3589e+01)\n",
      "Epoch: [1][ 60/390]\tTime  0.092 ( 0.103)\tData  0.002 ( 0.014)\tLinearAcc@1 100.00 ( 47.64)\tLinearAcc@5 100.00 ( 77.06)\tLoss 0.0000e+00 (2.9053e+01)\n",
      "Epoch: [1][ 70/390]\tTime  0.090 ( 0.102)\tData  0.001 ( 0.012)\tLinearAcc@1 100.00 ( 55.02)\tLinearAcc@5 100.00 ( 80.29)\tLoss 0.0000e+00 (2.4961e+01)\n",
      "Epoch: [1][ 80/390]\tTime  0.091 ( 0.100)\tData  0.002 ( 0.011)\tLinearAcc@1   0.00 ( 57.02)\tLinearAcc@5   0.00 ( 79.18)\tLoss 7.6056e+01 (2.4623e+01)\n",
      "Epoch: [1][ 90/390]\tTime  0.091 ( 0.099)\tData  0.002 ( 0.010)\tLinearAcc@1   0.00 ( 50.76)\tLinearAcc@5 100.00 ( 78.17)\tLoss 2.1759e+01 (2.7496e+01)\n",
      "Epoch: [1][100/390]\tTime  0.092 ( 0.099)\tData  0.002 ( 0.009)\tLinearAcc@1 100.00 ( 53.65)\tLinearAcc@5 100.00 ( 80.33)\tLoss 0.0000e+00 (2.4967e+01)\n",
      "Epoch: [1][110/390]\tTime  0.092 ( 0.098)\tData  0.002 ( 0.008)\tLinearAcc@1 100.00 ( 57.83)\tLinearAcc@5 100.00 ( 82.10)\tLoss 0.0000e+00 (2.2718e+01)\n",
      "Epoch: [1][120/390]\tTime  0.091 ( 0.097)\tData  0.002 ( 0.008)\tLinearAcc@1   0.00 ( 58.16)\tLinearAcc@5   0.00 ( 80.43)\tLoss 6.9787e+01 (2.3118e+01)\n",
      "Epoch: [1][130/390]\tTime  0.091 ( 0.097)\tData  0.002 ( 0.007)\tLinearAcc@1   0.00 ( 53.72)\tLinearAcc@5 100.00 ( 81.51)\tLoss 1.0434e+01 (2.4495e+01)\n",
      "Epoch: [1][140/390]\tTime  0.091 ( 0.097)\tData  0.001 ( 0.007)\tLinearAcc@1 100.00 ( 56.29)\tLinearAcc@5 100.00 ( 82.82)\tLoss 0.0000e+00 (2.2774e+01)\n",
      "Epoch: [1][150/390]\tTime  0.092 ( 0.096)\tData  0.002 ( 0.007)\tLinearAcc@1 100.00 ( 59.19)\tLinearAcc@5 100.00 ( 83.96)\tLoss 0.0000e+00 (2.1266e+01)\n",
      "Epoch: [1][160/390]\tTime  0.091 ( 0.096)\tData  0.002 ( 0.006)\tLinearAcc@1   0.00 ( 58.77)\tLinearAcc@5 100.00 ( 83.25)\tLoss 6.6031e+01 (2.2033e+01)\n",
      "Epoch: [1][170/390]\tTime  0.091 ( 0.096)\tData  0.002 ( 0.006)\tLinearAcc@1   0.00 ( 55.34)\tLinearAcc@5 100.00 ( 84.23)\tLoss 2.2272e+00 (2.2768e+01)\n",
      "Epoch: [1][180/390]\tTime  0.091 ( 0.095)\tData  0.002 ( 0.006)\tLinearAcc@1 100.00 ( 57.80)\tLinearAcc@5 100.00 ( 85.10)\tLoss 0.0000e+00 (2.1510e+01)\n",
      "Epoch: [1][190/390]\tTime  0.091 ( 0.095)\tData  0.002 ( 0.006)\tLinearAcc@1 100.00 ( 60.01)\tLinearAcc@5 100.00 ( 85.88)\tLoss 0.0000e+00 (2.0384e+01)\n",
      "Epoch: [1][200/390]\tTime  0.092 ( 0.095)\tData  0.002 ( 0.005)\tLinearAcc@1   0.00 ( 59.17)\tLinearAcc@5 100.00 ( 86.58)\tLoss 6.1958e+01 (2.1322e+01)\n",
      "Epoch: [1][210/390]\tTime  0.092 ( 0.095)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 56.84)\tLinearAcc@5 100.00 ( 87.22)\tLoss 3.1021e-03 (2.1661e+01)\n",
      "Epoch: [1][220/390]\tTime  0.091 ( 0.095)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 58.80)\tLinearAcc@5 100.00 ( 87.80)\tLoss 0.0000e+00 (2.0681e+01)\n",
      "Epoch: [1][230/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 60.58)\tLinearAcc@5 100.00 ( 88.33)\tLoss 0.0000e+00 (1.9786e+01)\n",
      "Epoch: [1][240/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1   0.00 ( 59.47)\tLinearAcc@5 100.00 ( 88.81)\tLoss 6.0013e+01 (2.0881e+01)\n",
      "Epoch: [1][250/390]\tTime  0.092 ( 0.094)\tData  0.002 ( 0.005)\tLinearAcc@1 100.00 ( 57.89)\tLinearAcc@5 100.00 ( 89.26)\tLoss 3.7073e-05 (2.1048e+01)\n",
      "Epoch: [1][260/390]\tTime  0.092 ( 0.094)\tData  0.001 ( 0.005)\tLinearAcc@1 100.00 ( 59.51)\tLinearAcc@5 100.00 ( 89.67)\tLoss 0.0000e+00 (2.0242e+01)\n",
      "Epoch: [1][270/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 61.00)\tLinearAcc@5 100.00 ( 90.05)\tLoss 0.0000e+00 (1.9495e+01)\n",
      "Epoch: [1][280/390]\tTime  0.092 ( 0.094)\tData  0.002 ( 0.004)\tLinearAcc@1   0.00 ( 59.70)\tLinearAcc@5 100.00 ( 90.40)\tLoss 5.3596e+01 (2.0585e+01)\n",
      "Epoch: [1][290/390]\tTime  0.092 ( 0.094)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 58.68)\tLinearAcc@5 100.00 ( 90.73)\tLoss 4.5635e-08 (2.0522e+01)\n",
      "Epoch: [1][300/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 60.05)\tLinearAcc@5 100.00 ( 91.04)\tLoss 0.0000e+00 (1.9840e+01)\n",
      "Epoch: [1][310/390]\tTime  0.091 ( 0.094)\tData  0.001 ( 0.004)\tLinearAcc@1 100.00 ( 61.33)\tLinearAcc@5 100.00 ( 91.33)\tLoss 0.0000e+00 (1.9202e+01)\n",
      "Epoch: [1][320/390]\tTime  0.092 ( 0.094)\tData  0.002 ( 0.004)\tLinearAcc@1   0.00 ( 59.89)\tLinearAcc@5 100.00 ( 91.60)\tLoss 4.7992e+01 (2.0303e+01)\n",
      "Epoch: [1][330/390]\tTime  0.092 ( 0.094)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 59.29)\tLinearAcc@5 100.00 ( 91.85)\tLoss 1.8626e-09 (2.0123e+01)\n",
      "Epoch: [1][340/390]\tTime  0.091 ( 0.094)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 60.48)\tLinearAcc@5 100.00 ( 92.09)\tLoss 0.0000e+00 (1.9533e+01)\n",
      "Epoch: [1][350/390]\tTime  0.092 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 61.61)\tLinearAcc@5 100.00 ( 92.32)\tLoss 0.0000e+00 (1.8977e+01)\n",
      "Epoch: [1][360/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1   0.00 ( 60.06)\tLinearAcc@5 100.00 ( 91.30)\tLoss 5.2772e+01 (2.0345e+01)\n",
      "Epoch: [1][370/390]\tTime  0.091 ( 0.093)\tData  0.001 ( 0.004)\tLinearAcc@1 100.00 ( 59.52)\tLinearAcc@5 100.00 ( 91.53)\tLoss 0.0000e+00 (2.0241e+01)\n",
      "Epoch: [1][380/390]\tTime  0.091 ( 0.093)\tData  0.002 ( 0.004)\tLinearAcc@1 100.00 ( 60.58)\tLinearAcc@5 100.00 ( 91.76)\tLoss 0.0000e+00 (1.9710e+01)\n",
      "Test: [ 0/78]\tTime  0.754 ( 0.754)\tData  0.000 ( 0.000)\tLinearAcc@1   8.59 (  8.59)\tLinearAcc@5  53.91 ( 53.91)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [10/78]\tTime  0.087 ( 0.147)\tData  0.000 ( 0.000)\tLinearAcc@1  13.28 ( 10.51)\tLinearAcc@5  52.34 ( 50.99)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [20/78]\tTime  0.087 ( 0.119)\tData  0.000 ( 0.000)\tLinearAcc@1   3.91 ( 10.01)\tLinearAcc@5  44.53 ( 50.30)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [30/78]\tTime  0.085 ( 0.108)\tData  0.000 ( 0.000)\tLinearAcc@1  11.72 ( 10.31)\tLinearAcc@5  55.47 ( 50.03)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [40/78]\tTime  0.087 ( 0.103)\tData  0.000 ( 0.000)\tLinearAcc@1   8.59 ( 10.21)\tLinearAcc@5  42.19 ( 49.79)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [50/78]\tTime  0.087 ( 0.100)\tData  0.000 ( 0.000)\tLinearAcc@1   6.25 (  9.96)\tLinearAcc@5  47.66 ( 49.85)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [60/78]\tTime  0.087 ( 0.098)\tData  0.000 ( 0.000)\tLinearAcc@1  12.50 (  9.89)\tLinearAcc@5  50.78 ( 49.73)\tLoss 0.0000e+00 (0.0000e+00)\n",
      "Test: [70/78]\tTime  0.087 ( 0.097)\tData  0.000 ( 0.000)\tLinearAcc@1  11.72 (  9.87)\tLinearAcc@5  50.00 ( 49.78)\tLoss 0.0000e+00 (0.0000e+00)\n",
      " * LinearAcc@1 10.016 LinearAcc@5 49.990\n",
      "Epoch: [2][  0/390]\tTime  0.867 ( 0.867)\tData  0.778 ( 0.778)\tLinearAcc@1   0.00 (  0.00)\tLinearAcc@5   0.00 (  0.00)\tLoss 8.3671e+01 (8.3671e+01)\n",
      "Epoch: [2][ 10/390]\tTime  0.091 ( 0.163)\tData  0.000 ( 0.073)\tLinearAcc@1   0.00 (  0.00)\tLinearAcc@5 100.00 ( 45.45)\tLoss 4.1550e+01 (6.7792e+01)\n",
      "Epoch: [2][ 20/390]\tTime  0.092 ( 0.128)\tData  0.002 ( 0.038)\tLinearAcc@1 100.00 ( 23.81)\tLinearAcc@5 100.00 ( 71.43)\tLoss 0.0000e+00 (3.9881e+01)\n",
      "Epoch: [2][ 30/390]\tTime  0.091 ( 0.116)\tData  0.002 ( 0.026)\tLinearAcc@1 100.00 ( 48.39)\tLinearAcc@5 100.00 ( 80.65)\tLoss 0.0000e+00 (2.7016e+01)\n",
      "Epoch: [2][ 40/390]\tTime  0.091 ( 0.110)\tData  0.002 ( 0.020)\tLinearAcc@1   0.00 ( 56.25)\tLinearAcc@5   0.00 ( 80.64)\tLoss 7.8431e+01 (2.4145e+01)\n",
      "Epoch: [2][ 50/390]\tTime  0.091 ( 0.107)\tData  0.002 ( 0.017)\tLinearAcc@1   0.00 ( 45.22)\tLinearAcc@5 100.00 ( 79.49)\tLoss 2.9889e+01 (3.0554e+01)\n",
      "Epoch: [2][ 60/390]\tTime  0.091 ( 0.104)\tData  0.002 ( 0.014)\tLinearAcc@1 100.00 ( 49.28)\tLinearAcc@5 100.00 ( 82.85)\tLoss 0.0000e+00 (2.6245e+01)\n",
      "Epoch: [2][ 70/390]\tTime  0.091 ( 0.102)\tData  0.002 ( 0.012)\tLinearAcc@1 100.00 ( 56.43)\tLinearAcc@5 100.00 ( 85.27)\tLoss 0.0000e+00 (2.2548e+01)\n",
      "Epoch: [2][ 80/390]\tTime  0.092 ( 0.101)\tData  0.002 ( 0.011)\tLinearAcc@1   0.00 ( 58.26)\tLinearAcc@5   0.00 ( 83.54)\tLoss 7.5463e+01 (2.2484e+01)\n",
      "Epoch: [2][ 90/390]\tTime  0.093 ( 0.100)\tData  0.002 ( 0.010)\tLinearAcc@1   0.00 ( 51.85)\tLinearAcc@5 100.00 ( 83.96)\tLoss 2.0859e+01 (2.5501e+01)\n",
      "Epoch: [2][100/390]\tTime  0.091 ( 0.099)\tData  0.002 ( 0.009)\tLinearAcc@1 100.00 ( 54.64)\tLinearAcc@5 100.00 ( 85.55)\tLoss 0.0000e+00 (2.3151e+01)\n",
      "Epoch: [2][110/390]\tTime  0.091 ( 0.098)\tData  0.002 ( 0.009)\tLinearAcc@1 100.00 ( 58.73)\tLinearAcc@5 100.00 ( 86.85)\tLoss 0.0000e+00 (2.1065e+01)\n",
      "Epoch: [2][120/390]\tTime  0.092 ( 0.098)\tData  0.002 ( 0.008)\tLinearAcc@1   0.00 ( 58.99)\tLinearAcc@5  89.06 ( 85.54)\tLoss 7.0591e+01 (2.1627e+01)\n",
      "Epoch: [2][130/390]\tTime  0.091 ( 0.097)\tData  0.002 ( 0.008)\tLinearAcc@1   0.00 ( 54.48)\tLinearAcc@5 100.00 ( 86.64)\tLoss 1.1250e+01 (2.3180e+01)\n",
      "Epoch: [2][140/390]\tTime  0.092 ( 0.097)\tData  0.002 ( 0.007)\tLinearAcc@1 100.00 ( 57.00)\tLinearAcc@5 100.00 ( 87.59)\tLoss 0.0000e+00 (2.1558e+01)\n",
      "Epoch: [2][150/390]\tTime  0.092 ( 0.097)\tData  0.002 ( 0.007)\tLinearAcc@1 100.00 ( 59.85)\tLinearAcc@5 100.00 ( 88.41)\tLoss 0.0000e+00 (2.0130e+01)\n",
      "Epoch: [2][160/390]\tTime  0.091 ( 0.096)\tData  0.002 ( 0.006)\tLinearAcc@1   0.00 ( 59.39)\tLinearAcc@5 100.00 ( 89.11)\tLoss 6.6410e+01 (2.0979e+01)\n",
      "Epoch: [2][170/390]\tTime  0.092 ( 0.096)\tData  0.002 ( 0.006)\tLinearAcc@1   0.00 ( 55.92)\tLinearAcc@5 100.00 ( 89.75)\tLoss 2.5972e+00 (2.1797e+01)\n",
      "Epoch: [2][180/390]\tTime  0.092 ( 0.096)\tData  0.002 ( 0.006)\tLinearAcc@1 100.00 ( 58.36)\tLinearAcc@5 100.00 ( 90.31)\tLoss 0.0000e+00 (2.0593e+01)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_train\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m N_EPOCH \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_linear(\n\u001b[1;32m      4\u001b[0m     run_name\u001b[39m=\u001b[39;49mrun_name,\n\u001b[1;32m      5\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m      6\u001b[0m     val_loader\u001b[39m=\u001b[39;49mval_loader,\n\u001b[1;32m      7\u001b[0m     feature_extractor\u001b[39m=\u001b[39;49mresnet50_cifar,\n\u001b[1;32m      8\u001b[0m     n_epoch\u001b[39m=\u001b[39;49mN_EPOCH,\n\u001b[1;32m      9\u001b[0m     finetune_extractor\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m )\n",
      "Cell \u001b[0;32mIn[17], line 25\u001b[0m, in \u001b[0;36mtrain_linear\u001b[0;34m(run_name, train_loader, val_loader, feature_extractor, n_epoch, finetune_extractor)\u001b[0m\n\u001b[1;32m     17\u001b[0m     optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(\n\u001b[1;32m     18\u001b[0m         linear_classifier\u001b[39m.\u001b[39mparameters(),\n\u001b[1;32m     19\u001b[0m         lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,\n\u001b[1;32m     20\u001b[0m         momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epoch):\n\u001b[1;32m     24\u001b[0m     \u001b[39m# train\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     train_cifar10_classifier(\n\u001b[1;32m     26\u001b[0m         train_loader,\n\u001b[1;32m     27\u001b[0m         feature_extractor,\n\u001b[1;32m     28\u001b[0m         linear_classifier,\n\u001b[1;32m     29\u001b[0m         optimizer,\n\u001b[1;32m     30\u001b[0m         epoch,\n\u001b[1;32m     31\u001b[0m         device,\n\u001b[1;32m     32\u001b[0m         finetune_extractor,\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     \u001b[39m# validate\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m validate_cifar10_classifier(\n\u001b[1;32m     37\u001b[0m         val_loader, feature_extractor, linear_classifier, device\n\u001b[1;32m     38\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[15], line 36\u001b[0m, in \u001b[0;36mtrain_cifar10_classifier\u001b[0;34m(train_loader, model, linear_classifier, optimizer, epoch, device, finetune_extractor)\u001b[0m\n\u001b[1;32m     33\u001b[0m logits \u001b[39m=\u001b[39m linear_classifier(hs)\n\u001b[1;32m     34\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, target)\n\u001b[0;32m---> 36\u001b[0m avg_meters[\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39;49mitem(), batch_size)\n\u001b[1;32m     38\u001b[0m acc1, acc5 \u001b[39m=\u001b[39m accuracy(logits, target, topk\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[1;32m     39\u001b[0m top1\u001b[39m.\u001b[39mupdate(acc1[\u001b[39m0\u001b[39m], batch_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_name = \"test_train\"\n",
    "N_EPOCH = 100\n",
    "train_linear(\n",
    "    run_name=run_name,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    feature_extractor=resnet50_cifar,\n",
    "    n_epoch=N_EPOCH,\n",
    "    finetune_extractor=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbd214206844d032e7ca841ed0642b378d73189af10877ee326de88a48fbee75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
