{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "from functools import reduce\n",
    "from typing import Union\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание эксперемента\n",
    "В этом примере мы применим методы визуализации для анализа классификационной модели.  \n",
    "Пусть у нас есть набор данных caltech101, для которого мы хотим получить хороший классификатор. \n",
    " \n",
    "Этот ноутбук состоит из следующих этапов:\n",
    "1. Подготовка данных\n",
    "2. Загрузка предобученной на ImageNet модели, которую мы будем дообучать\n",
    "3. Дообучение модели\n",
    "4. Анализ ошибок модели на валидации с помощью методов визуализации, рассмотренных на лекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's have a cell with global hyperparameters for the CNNs in this notebook\n",
    "\n",
    "# Path to a directory with image dataset and subfolders for training, validation and final testing\n",
    "DATA_PATH = \"../datasets\"  # PATH TO THE DATASET\n",
    "\n",
    "# Number of threads for data loader\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Image size: even though image sizes are bigger than 96, we use this to speed up training\n",
    "SIZE_H = SIZE_W = 224\n",
    "N_CHANNELS = 3\n",
    "\n",
    "# Number of classes in the dataset\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Epochs: number of passes over the training data, we use it this small to reduce training babysitting time\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# Batch size: for batch gradient descent optimization, usually selected as 2**K elements\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Images mean and std channelwise\n",
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Last layer (embeddings) size for CNN models\n",
    "EMBEDDING_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((SIZE_H, SIZE_W)),  # scaling images to fixed size\n",
    "        transforms.ToTensor(),  # converting to tensors\n",
    "        transforms.Lambda(\n",
    "            lambda x: torch.cat([x, x, x], 0) if x.shape[0] == 1 else x\n",
    "        ),  # treat gray images\n",
    "        transforms.Normalize(image_mean, image_std),  # normalize image data per-channel\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split it into train and val\n",
    "caltech101 = torchvision.datasets.Caltech101(\n",
    "    root=DATA_PATH, download=True, transform=transformer\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(caltech101, [7000, 1677])\n",
    "\n",
    "caltech101_unchanged = torchvision.datasets.Caltech101(root=DATA_PATH, download=True)\n",
    "torch.manual_seed(0)\n",
    "train_dataset_unchanged, val_dataset_unchanged = torch.utils.data.random_split(\n",
    "    caltech101_unchanged, [7000, 1677]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val = len(train_dataset), len(val_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Загрузка предобученной на ImageNet модели VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" VGG16\n",
    "    \"\"\"\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Дообучение модели VGG16 на нашем датасете\n",
    "Если у вас не хватает видео памяти, попробуйте уменьшить размер батча *BATCH_SIZE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, val_loader):\n",
    "    val_accuracy = []\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        # move data to target device\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        # compute logits\n",
    "        logits = model(X_batch)\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        val_accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, opt, n_epochs):\n",
    "    \"\"\"\n",
    "    model: нейросеть для обучения,\n",
    "    train_loader, val_loader: загрузчики данных\n",
    "    loss_fn: целевая метрика (которую будем оптимизировать)\n",
    "    opt: оптимизатор (обновляет веса нейросети)\n",
    "    n_epochs: кол-во эпох, полных проходов датасета\n",
    "    \"\"\"\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True)  # enable dropout / batch_norm training behavior\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # move data to target device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # train on batch: compute loss, calc grads, perform optimizer step and zero the grads\n",
    "            opt.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = loss_fn(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            #             torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            opt.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        model.train(False)  # disable dropout / use averages for batch_norm\n",
    "        val_accuracy += compute_accuracy(model, val_loader)\n",
    "\n",
    "        # print the results for this epoch:\n",
    "        print(f\"Epoch {epoch + 1} of {n_epochs} took {time.time() - start_time:.3f}s\")\n",
    "\n",
    "        train_loss_value = np.mean(train_loss[-n_train // BATCH_SIZE :])\n",
    "        val_accuracy_value = np.mean(val_accuracy[-n_val // BATCH_SIZE :]) * 100\n",
    "\n",
    "        print(f\"  training loss (in-iteration): \\t{train_loss_value:.6f}\")\n",
    "        print(f\"  validation accuracy: \\t\\t\\t{val_accuracy_value:.2f} %\")\n",
    "\n",
    "    return train_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# train_loss, val_accuracy = train_model(\n",
    "#     model_ft, train_loader, val_loader, loss_fn, optimizer_ft, EPOCH_NUM\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_ft.state_dict(), '../trained_models/vgg16.pt')\n",
    "\n",
    "# load model\n",
    "\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device)\n",
    "model_ft.load_state_dict(torch.load(\"../trained_models/vgg16.pt\"))\n",
    "model_ft.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute val accuracy\n",
    "\n",
    "np.mean(compute_accuracy(model_ft, val_loader)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Анализ предсказаний нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация одного примера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../seminars/\")\n",
    "from l5_visualization.scripts.visualize_cnn import get_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 44\n",
    "image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "get_explanations(\n",
    "    model_ft,\n",
    "    image_transformed,\n",
    "    image_unchanged,\n",
    "    caltech101.categories[image_category],\n",
    "    image_category,\n",
    "    Path(\"../outputs/test.png\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.eval()\n",
    "out_dir = Path(\"../outputs/caltech101_vis/\")\n",
    "for image_index in tqdm(range(len(val_dataset_unchanged))):\n",
    "    image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "    true_category_name = caltech101.categories[image_category]\n",
    "    # prepare image\n",
    "    image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "    # get class scores\n",
    "    class_scores = model_ft(image_transformed)\n",
    "    class_scores = class_scores.detach().cpu().numpy()[0]\n",
    "    predicted_class = np.argmax(class_scores)\n",
    "\n",
    "    formated_image_index = str(image_index).zfill(4)\n",
    "\n",
    "    if predicted_class == image_category:\n",
    "        # right classified image. Save its visualization to foulder with class category name\n",
    "\n",
    "        save_path = out_dir / Path(\n",
    "            f\"true/{true_category_name}_id_{image_category}/{formated_image_index}.png\"\n",
    "        )\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path,\n",
    "        )\n",
    "    else:\n",
    "        # misclassified image. Save vis with respect to true and predicted classes\n",
    "        predicted_category_name = caltech101.categories[predicted_class]\n",
    "        predicted_class_score = str(round(class_scores[predicted_class], 3))\n",
    "        true_class_score = str(round(class_scores[image_category], 3))\n",
    "        save_path_predicted_vis = out_dir / Path(\n",
    "            f\"mis_class/{predicted_category_name}_id_{predicted_class}_predicted\"\n",
    "        )\n",
    "        save_path_predicted_vis /= Path(\n",
    "            f\"{formated_image_index}_predicted_category({predicted_category_name})_score_{predicted_class_score}_vis.png\"\n",
    "        )\n",
    "\n",
    "        save_path_true_vis = out_dir / Path(\n",
    "            f\"mis_class/{predicted_category_name}_id_{predicted_class}_predicted\"\n",
    "        )\n",
    "        save_path_true_vis /= Path(\n",
    "            f\"{formated_image_index}_true_category({true_category_name})_score_{true_class_score}_vis.png\"\n",
    "        )\n",
    "\n",
    "        # predicted target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            predicted_class,\n",
    "            save_path_predicted_vis,\n",
    "        )\n",
    "        # true target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path_true_vis,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bada2b9662873f86490f179f527b07823fa53d955971105c0ff3ba75e58f0801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
