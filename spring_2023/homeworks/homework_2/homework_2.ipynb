{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №2\n",
    "\n",
    "В данном задании вам предлагаются 3 задачи:\n",
    "\n",
    "#### 1. Обучение небольшой ArcFace модели (0.4 балла)\n",
    "Вам предлагается использовать функцию потерь [ArcFace](https://arxiv.org/abs/1801.07698) для решения задачи Metric Learning в распознавании лиц.  \n",
    "Подробное описание задачи вы можете найти в соответствующем разделе домашнего задания.  \n",
    "**Ваша цель:** Обучить модели для предсказания дискриминативных представлений изображений, $\\textbf{z}\\in\\mathbb{R}^{2}$, с помощью функции потерь SoftMax и функции из статьи [ArcFace](https://arxiv.org/abs/1801.07698) (см. описание задачи 1). Модели необходимо обучить на 8-ми самых многочисленных классах из набора изображений лиц [MS1M-ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_).  Затем нужно изобразить полученные представления на двух рисунках (один для SoftMax другой для ArcFace функций потерь). \n",
    "\n",
    "Рисунок для SoftMax функции будет вам дан. Вспомогательный код для загрузки данных и для обучения модели с помощью SoftMax функции потерь вы можете найти в разделе с заданием.  \n",
    "Помогает ли функция потерь ArcFace для улучшения дикриминативных способностей векторов представлений?\n",
    "\n",
    "#### 2. Использование предобученной модели для решения задачи верификации лиц (0.4 балла)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1. Обучение небольшой ArcFace модели (0.4 балла)\n",
    "\n",
    "При решении задачи распознавания лиц возникает необходимость ответить на вопрос: изображен ли на двух разных картинках один и тот же человек или нет?  \n",
    "На этот вопрос можно ответить с помощью функции расстояния между изображениями, учитывающую абстрактную семантическую информацию.  \n",
    "\n",
    "\n",
    "Современные методы распознавания лиц используют большие наборы данных, содержащие изображения разных людей, для обучения нейросетей, вычисляющий осмысленные вектора представлений изображений лиц.\n",
    "Для каждого изображения лица человека $\\textbf{x}$ с помощью обучаемой функции $f_{\\theta}(*)$ вычисляется вектор представления $\\textbf{x}$:\n",
    "$$\n",
    "\\textbf{z} = f_{\\theta}(\\textbf{x})\n",
    "$$\n",
    "Параметры $\\theta$ подбираются так, чтобы расстояние между векторами представлений разных людей было велико, а между представлениями изображений одного и того же человека было низко:\n",
    "$$\n",
    "d(\\textbf{z}_i,\\textbf{z}_j)>>d(\\textbf{z}_i,\\textbf{z}'_i)\n",
    "$$\n",
    "где $\\mathbf{id}(\\textbf{x}_i)=\\mathbf{id}(\\textbf{x}'_i),\\,\\mathbf{id}(\\textbf{x}_i)\\neq\\mathbf{id}(\\textbf{x}_j)$, $d$ - простая функция расстояния, e.g косинусное расстояние:\n",
    "$$\n",
    "d(\\textbf{z}_i,\\textbf{z}_j) = -\\frac{\\langle\\textbf{z}_i,\\textbf{z}_j\\rangle}{\\left\\lVert\\textbf{z}_i\\right\\rVert\\left\\lVert\\textbf{z}_j\\right\\rVert}\n",
    "$$\n",
    "\n",
    "Таким образом мы можем получить осмысленную функцию расстояния между изображениями лиц.  \n",
    "\n",
    "Для получения искомых представлений можно решать задачу классификации на большом наборе данных с изображениями разных людей.  \n",
    "Затем можно использовать вектор перед линейным слоем как вектор представления лица. \n",
    "Здесь каждый человек в наборе данных рассматривается как отдельный класс, а множество изображений его лица рассматриваются как представители класса.  \n",
    "Softmax функция потерь для много-классовой классификации:\n",
    "\n",
    "$$\n",
    "    L = -\\frac{1}{N}\\sum_{i=1}^N\\log\\frac{e^{W_{y_i}^T\\textbf{z}_i + b_{y_i}}}{\\sum_{j=1}^ne^{W_{j}^T\\textbf{z}_i + b_{j}}}\n",
    "$$\n",
    "$\\textbf{z}_i$ - представление изображения, $N$ - число изображений в мини батче, $W_j, b_j$ - веса гиперплоскости для каждого класса.\n",
    "\n",
    "Если мы ограничим $x$, чтобы он имел ограниченную норму s, и $W_j$ имел норму 1, а $b_j=0$, тогда функция потерь может быть переписана в следующей форме:\n",
    "$$\n",
    "L = -\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\log\n",
    "\\frac{e^{s\\cos\\theta_{y_i}}}\n",
    "{e^{s\\cos\\theta_{y_i}} + \\sum_{j\\neq y_i}^ne^{s\\cos\\theta_{j}}}\n",
    "$$\n",
    "где $\\theta_j$ - угол между представлением $i$-го изображения и вектором $W_j$ указывающим на центр $j$-ой класса.\n",
    "\n",
    "Мы можем видеть, что здесь максимизируется косинус угла между представлением и соответствующим вектором класса.\n",
    "\n",
    "В идеале мы хотим минимизировать сам угол, а не максимизировать его косинус, потому что угол лучше соответствует близости в пространстве представлений.\n",
    "\n",
    "Предлагается [ArcFace](https://arxiv.org/abs/1801.07698) функция потерь для непосредственной минимизации угла:\n",
    "$$\n",
    "L = -\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\log\n",
    "\\frac{e^{s\\cos(\\theta_{y_i}+m)}}\n",
    "{e^{s\\cos(\\theta_{y_i}+m)} + \\sum_{j\\neq y_i}^ne^{s\\cos\\theta_{j}}}\n",
    "$$\n",
    "где $m=0.5$.\n",
    "\n",
    "**Ваша цель:** Обучить модели для предсказания дискриминативных представлений изображений, $\\textbf{z}\\in\\mathbb{R}^{2}$, с помощью функции потерь SoftMax и функции из статьи [ArcFace](https://arxiv.org/abs/1801.07698) (см. описание задачи 1). Модели необходимо обучить на 8-ми самых многочисленных классах из набора изображений лиц [MS1M-ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_).  Затем нужно изобразить полученные представления на двух рисунках (один для SoftMax другой для ArcFace функций потерь). \n",
    "\n",
    "Рисунок для SoftMax функции будет вам дан. Вспомогательный код для загрузки данных и для обучения модели с помощью SoftMax функции потерь вы можете найти в разделе с заданием.  \n",
    "Помогает ли функция потерь ArcFace для улучшения дикриминативных способностей векторов представлений?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка данных\n",
    "\n",
    "Загрузите набор MS1M-ArcFace и распакуйте данные текущей дериктории:\n",
    "https://drive.google.com/file/d/1SXS4-Am3bsKSK615qbYdbA_FMVh3sAvR/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import mxnet as mx\n",
    "import numbers\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MXFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_labels, test=False):\n",
    "        \"\"\"\n",
    "        ArcFace loader\n",
    "        https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/dataset.py\n",
    "        \"\"\"\n",
    "        super(MXFaceDataset, self).__init__()\n",
    "\n",
    "        self.test = test\n",
    "        if self.test:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "        self.root_dir = root_dir\n",
    "        path_imgrec = os.path.join(root_dir, \"train.rec\")\n",
    "        path_imgidx = os.path.join(root_dir, \"train.idx\")\n",
    "        self.imgrec = mx.recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, \"r\")\n",
    "        s = self.imgrec.read_idx(0)\n",
    "        header, _ = mx.recordio.unpack(s)\n",
    "        if header.flag > 0:\n",
    "            self.header0 = (int(header.label[0]), int(header.label[1]))\n",
    "            self.imgidx = np.array(range(1, int(header.label[0])))\n",
    "        else:\n",
    "            self.imgidx = np.array(list(self.imgrec.keys))\n",
    "        labels_path = Path(root_dir) / \"labels.npy\"\n",
    "        if labels_path.is_file():\n",
    "            self.labels = np.load(labels_path)\n",
    "        else:\n",
    "            print('Listing labels...')\n",
    "            labels = []\n",
    "            for i in tqdm(range(len(self.imgidx))):\n",
    "                idx = self.imgidx[i]\n",
    "                s = self.imgrec.read_idx(idx)\n",
    "                header, img = mx.recordio.unpack(s)\n",
    "                label = header.label\n",
    "                labels.append(int(label))\n",
    "            self.labels = np.array(labels)\n",
    "            np.save(labels_path, self.labels)\n",
    "        unique_labels, unique_counts = np.unique(self.labels, return_counts=True)\n",
    "        top_ids = np.argsort(unique_counts)[::-1][:num_labels]\n",
    "        self.top_labels = unique_labels[top_ids]\n",
    "\n",
    "        self.label_map = dict(\n",
    "            zip(self.top_labels.tolist(), np.arange(len(self.top_labels)))\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.imgidx[index]\n",
    "        s = self.imgrec.read_idx(idx)\n",
    "        header, img = mx.recordio.unpack(s)\n",
    "        label = header.label\n",
    "        if not isinstance(label, numbers.Number):\n",
    "            label = label[0]\n",
    "        label = self.label_map[int(label)]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        sample = mx.image.imdecode(img).asnumpy()\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.test:\n",
    "            return sample\n",
    "        else:\n",
    "            return sample, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_people = 4\n",
    "ms1m_dataset = MXFaceDataset(\"data/ms1m/\", num_labels=num_people)\n",
    "people_ids = np.where(np.isin(ms1m_dataset.labels, ms1m_dataset.top_labels))[0]\n",
    "people_set = torch.utils.data.Subset(ms1m_dataset, people_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(people_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iresnet import iresnet50_normalized\n",
    "\n",
    "\n",
    "class MetricLearningModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, backbone: torch.nn.Module, loss: torch.nn.Module, num_labels: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.loss = loss\n",
    "        self.softmax_weights = torch.nn.Parameter(\n",
    "            torch.normal(0, 1, size=(2, num_labels)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_outputs = self.backbone(x)\n",
    "        features = backbone_outputs[\"feature\"]\n",
    "        norm_softmax_weights = F.normalize(self.softmax_weights, dim=0)\n",
    "        logits = torch.matmul(features, norm_softmax_weights)\n",
    "        return features, logits\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        features, logits = self(images)\n",
    "        loss = self.loss(logits, labels)\n",
    "        self.log(\"train_loss\", loss.item(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + [self.softmax_weights]\n",
    "        optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=5e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "        }\n",
    "\n",
    "\n",
    "backbone_model = iresnet50_normalized(num_features=2)\n",
    "softmax_loss = torch.nn.CrossEntropyLoss()\n",
    "arcface_model = MetricLearningModel(backbone_model, softmax_loss, num_labels=num_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 10\n",
    "max_epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA A10') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name     | Type             | Params\n",
      "----------------------------------------------\n",
      "0 | backbone | IResNetNorm      | 30.8 M\n",
      "1 | loss     | CrossEntropyLoss | 0     \n",
      "----------------------------------------------\n",
      "30.8 M    Trainable params\n",
      "2         Non-trainable params\n",
      "30.8 M    Total params\n",
      "123.178   Total estimated model params size (MB)\n",
      "/home/erlygin/miniconda/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9/9 [00:20<00:00,  2.24s/it, v_num=10, train_loss=0.771]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9/9 [00:21<00:00,  2.36s/it, v_num=10, train_loss=0.771]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = Trainer(max_epochs=max_epochs, default_root_dir=\"outputs/softmax_train\")\n",
    "train_dataloader = DataLoader(\n",
    "    people_set,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "trainer.fit(arcface_model, train_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление представлений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = iresnet50_normalized(num_features=2)\n",
    "softmax_loss = torch.nn.CrossEntropyLoss()\n",
    "checkpoint_path = \"/app/spring_2023/homeworks/homework_2/outputs/softmax_train/lightning_logs/version_10/checkpoints/epoch=4-step=45.ckpt\"\n",
    "softmax_model = MetricLearningModel.load_from_checkpoint(\n",
    "    num_labels=num_people,\n",
    "    backbone=backbone_model,\n",
    "    loss=softmax_loss,\n",
    "    checkpoint_path=checkpoint_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_features(model):\n",
    "    test_dataloader = DataLoader(\n",
    "        people_set,\n",
    "        16,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    model.to(cuda)\n",
    "    # disable randomness, dropout, etc...\n",
    "    model.eval()\n",
    "    predicted_features = []\n",
    "    image_labels = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        images, labels = batch\n",
    "        images = images.to(cuda)\n",
    "        features, _ = model(images)\n",
    "        features = features.detach().cpu().numpy()\n",
    "        labels = labels.numpy()\n",
    "        predicted_features.append(features)\n",
    "        image_labels.append(labels)\n",
    "    predicted_features = np.concatenate(predicted_features)\n",
    "    image_labels = np.concatenate(image_labels)\n",
    "    return predicted_features, image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:18<00:00,  8.09it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_features, image_labels = predict_features(softmax_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "colors = list(mcolors.TABLEAU_COLORS)[:num_people]\n",
    "softmax_weights = softmax_model.softmax_weights.detach().cpu()\n",
    "softmax_weights = F.normalize(softmax_weights, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_classes = 1\n",
    "for i, (weight, color) in enumerate(zip(softmax_weights.T, colors)):\n",
    "    points = predicted_features[image_labels == i]\n",
    "    x, y = [0, weight[0]], [0, weight[1]]\n",
    "    plt.plot(x, y, marker=\"\", c=color)\n",
    "    plt.scatter(points[:, 0], points[:, 1], color=color, s=3)\n",
    "    if i == (max_classes - 1):\n",
    "        break\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "a = torch.nn.Parameter(\n",
    "            torch.tensor(a_np, dtype=float), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = F.normalize(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2425, 0.3714, 0.4472],\n",
       "        [0.9701, 0.9285, 0.8944]], dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "features = torch.tensor(features_np, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.],\n",
       "        [7., 8.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9., 12., 15.],\n",
       "        [19., 26., 33.],\n",
       "        [29., 40., 51.],\n",
       "        [39., 54., 69.]], dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(features, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.],\n",
       "        [7., 8.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_np = a_np / np.linalg.norm(a_np, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24253563, 0.9701425 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_np.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
